Привет!

Для меня честь, представить тебе свои работы. Спасибо за уделенное время!

Все проекты, описанные в этом репозитории были проверены код-ревьерами Яндекс Практикума*. 

**У каждого проекта есть своя цель: научиться использовать навыки полученные в теоретическом блоке модуля, а также получить практический опыт в сфере Data-Science, чтобы стать профессионалом.**

Точно знаю, если мне предстоит выполнить хотя бы немного похожу рабочую задачу, я открою модуль с теорией, перечитаю проект, и даже, если я делал что-то однажды и давно, у меня не возникнет проблем повторить это!
Если рабочая задача будет абсолютно новой для меня, это тоже не проблема. Как и со всеми проектами тут, я погружусь в тему, а потом сделаю все как надо! 

Этими работами, я показываю применение своих навыков на практике. Проекты не идеальны, но они прекрасно соотвествуют цели "научиться быть профеccионалом". 
Я не планирую переписывать старые работы. Вместо этого я получаю новые необходимые навыки. Например, 2 марта 2025 года, у меня в Практикуме начался первый учебный модуль по нейросетям. Пройти его невероятно важно и вдохновляюще.

**Вы можете быть уверены в том, что мои рабочие проекты будут сделаны идеально относительно требуемого результата и имеющегося времени!**

Это правда потому что:
1. Во первых, при выполнении рабочих проектов ставится другая цель, не научиться решать проблему, а получить "оптимальный результат". Это значит, что к итоговой работе в разы более высокие требования.
2. Во вторых, если потребуется внести правки, я без проблем сделаю это. Каждый проект получал обязательную и опциональную критику от ревьера (%). Я всегда вношу все обязательные исправления. 
3. В третьих, я использую весь опыт и критику при выполнении нового проекта. Вы можете проследить как растет уровень выполнения работы от последнего проекта, к первому (последний проект - самый верхний, первый проект - самый нижний). Всю полученную критику (в том числе и не обязательные исправления) я собираю у себя на доске в Figma 
Эта доска постоянно пополняется и уже содержит больше 30 исправлений моих частых ошибок от специалистов с опытом работы более 2 лет. При работе перед новым проектом я перечитываю ее, что позволяет мне не допускать ошибки. 
4. В четвертых, я написал себе подробный алгоритм работы над задачей машинного обучения / аналитики. Он содержит 10 блоков (например, оформление, знакомство с данными, отбор признаков, улучшение качества моделей) по 4 пункта в каждом блоке в среднем. Этот алгоритм позволяет мне ничего не забыть.
5. И наконец, у меня есть отличная обратная связь с моего последнего места работы! Около 6 месяцев я работал в проектах Дмитрия Зборовского. Раньше он был старшим вице-президентом по искусственному интеллекту и внутренней эффективности «Сбермаркета», а сейчас он руководит международным Data Science в Deliveroo. Дима отметил мое желание и умение развиваться с каждым разом, а также отличное структурирование и организацию моих работ.

(%) Кроме проектов с автоматической проверкой. Это проект по SQL и первая часть проекта "Предобработка данных"

<h2>Комментарии</h2>

<details>
<summary>Про последние изменения, внесенные в проекты</summary>
    Даже первые проекты были отредактированы 11.03.2025, изменения не включали в себя перепись кода и прочие желательные правки. Я добавил вывод версий библиотек и исправил отображение некоторых элементов. To Do не был дополнен полностью. 
</details>

<details>
  <summary>Комментарий для Сбера</summary>
    Обновляю информацию о проектах из резюме

    Анализ остатков был проведен в работе "Использование моделей линейной и логистической регрессии для предсказания вкуса и количества молока, у коров, планируемых к покупке", а не в работе "Большой проект по использованию sklearn". Проектов больше чем 9, а не 9, так как были добавлены проекты с автоматической проверкой
</details>

<details>
  <summary>Используемые термины</summary>
    В контексте этого репозитория спринт - это теоретический блок + проект по этому теоритическому блоку. Каждый проект, который вы видите в этом репозитории, был выполнен в рамках своего спринта. Т.е каждому проекту соответствует свой блок с теорией, который был пройден до выполнения проекта. В среднем на спринт уходит от 20 до 40 часов времени
</details>

<details>
  <summary>Качество написанных тесктов с точки зрения русского языка и легкости чтения</summary>
    Стараюсь быть максимально граммотным и понятным в своей речи. Более того, в 2023 году, я сдал ЕГЭ по русскому на 97 из 100 баллов. Тем не менее из-за быстрого написания тесктов, иногда отстутсвия пристальной проверки, в них могут содержаться грамматические и орфографические ошибки.
</details>



<h2>Портфолио</h2>

| Название работы | Описание работы | Технологии |
| --------------- | --------------- | ---------- |
| [Предсказание стоимости жилья в Калифорнии с помощью PySpark ⭐️](https://github.com/the13den/yandex_practicum_ds_pyspark_house_cost_pred_n10) | Используя PySpark, я предобработал данные о жилье в Калифорнии и создал pipeline с моделью линейной регрессии. Этот пайплайн подготавливает данные и предсказывает медианную стоимость жилья в районе. Я предложил множество рекомендаций, которые помогут улучшить точность прогнозов. Несмотря на небольшой объем работы, считаю её своей лучшей: она максимально качественно выполнена и хорошо структурирована. Если какой-либо этап, например, отбор признаков, не был выполнен, значит он не требовался по техническому заданию, или задача уже была сведена к решенной в предыдущих проектах. | `Python`, `Pandas`, `matplotlib`, `seaborn`, `pyspark`, `pyspark sql` |
| [Терминал. Git. Github](https://github.com/the13den/yandex_practicum_ds_terminal_operations_git_github_n9) | В этом проекте требовалось выложить на GitHub работу №5, сделать несколько коммитов и открыть несколько Issues. Также я повторил работу с файлами и каталогами через терминал, научился пользоваться Git через консоль и стал делать это регулярно. | `Терминал`, `Git`, `Github` |
| [Базовый SQL. Postegre SQL. срезы, агрегирование, объединения таблиц, подзапросы и CTE](https://github.com/the13den/yandex_practicum_ds_postegre_sql_basics_joins_subqueries_etc_n8) | Этот проект проверялся автоматически. В нём я написал 23 запроса по базе данных Startup Investments, опубликованной на популярной платформе Kaggle. Примеры запросов вы можете увидеть на странице проекта. Также в рамках этого спринта, в теоретическом блоке, который рассчитан на 20 часов, я написал более 100 запросов.  | `SQL`, `PostegreSQL` |
| [ML прогнозирование количества сырья в скважинах региона, расчет прибыли, выбор оптимального региона для разработки с помощью техники Bootstrap ](https://github.com/the13den/yandex_practicum_ds_bootrap_risk_modeling_ml_oil_product_prediction_for_oil_company_n7) | В этом проекте нам даны три региона, для каждого из которых предоставлен отдельный файл с записями о скважинах и их характеристиках. Необходимо выбрать регион для разработки, соответствующий требованиям бизнеса. Для решения задачи я загрузил и предобработал данные, затем подготовил их с помощью пайплайна. Поскольку информация о природе признаков не раскрыта, исследовательский анализ данных не проводился. Я обучил модель линейной регрессии, чтобы предсказать количество сырья в скважинах. После этого я расчитал прибыль. В конце с помощью техники Bootstrap я создал 1000 выборок по 500 скважин для каждого региона и рассчитал доверительные интервалы прибыли. На основе полученных результатов предложил оптимальный регион для разработки. | `Python`, `Pandas`, `matplotlib`, `Seaborn`, `Sklearn`, `Numpy`, `Bootstrap` |
| [Большой проект по предсказанию увольнений сотрудников и удовлетворенности через методы машинного обучения. Анализ факторов, влияющих на решение о расторжении трудового договора и вовлеченности сотрудников.](https://github.com/the13den/yandex_practicum_ds_big_projec_supervised_learning_employee_satisfaction_and_dismissal_prediction_n6) | В данной работе я решил две бизнес-задачи. Первая задача — построить модель, которая сможет предсказывать уровень удовлетворённости сотрудников на основе данных заказчика. Это важно для бизнеса, так как удовлетворённость сотрудников напрямую влияет на их отток, а предсказание оттока — одна из важнейших задач HR-аналитики. Вторая задача — создать модель, которая сможет прогнозировать увольнение сотрудника. Для решения задач я предобработал данные, провёл исследовательский анализ и выяснил, на какие категории делятся сотрудники. Затем я выявил факторы, влияющие на удовлетворённость и отток сотрудников. После этого отобрал признаки, проверил их на мультиколлинеарность и утечку данных. В конце я собрал два пайплайна машинного обучения с использованием sklearn, каждый с полным циклом подготовки данных. Пайплайн регрессии (Decision Tree Regressor) был оценен кастомной метрикой SMAPE. Пайплайн классификации был оценен — метрикой ROC-AUC. Результатом работы стали две модели для предсказания нужных характеристик, а также портрет уволившегося сотрудника | `Python`, `Pandas`, `matplotlib`, `seaborn`, `Sklearn`, `Numpy`, `phik` |
| [Предсказание снижения активности клиента интернет магазина с помощью четырех моделей классификации. Решение проблемы дисбаланса классов, использование пайплана и подбора гиперпараметров на кроссвалидации](https://github.com/the13den/yandex_practicum_ds_supervised_learning_user_activity_prediction_n5) | Для обратившейся компании решил следующую бизнес задачу. Нужно было промаркировать уровень финансовой активности постоянных покупателей. В компании принято выделять два уровня активности клиентов: «снизилась» (если клиент стал покупать меньше товаров) и «прежний уровень». В исследование также нужно было включить дополнительные данные от финансового департамента о прибыльности клиентов, а именно какой доход каждый покупатель приносил компании за последние три месяца. Используя результаты модели и данные о прибыльности клиентов, дополнительно, необходимо выделить сегменты покупателей и разработать персонализированные предложения для каждого сегмента. Я решал задачу следующим образом: сначала предобработал и исследовал данные, используя библиотеки Pandas и визуализации. Затем отобрал признаки и создал пайплайн с помощью библиотеки sklearn, который заполняет пропуски, кодирует категориальные признаки и масштабирует числовые данные. В пайплайне были использованы четыре модели: KNN, SVC, DTC и Logistic Regression. Лучшая модель и ее гиперпараметры, например, количество соседей для KNN и регуляризация для Logistic Regression, была выбрана с помощью рандомизированного поиска на кросс-валидации.  С помощью модели  я промаркировал уровень финансовой активности постоянных покупателей. Затем, используя SHAP-анализ, я интерпретировал модель, чтобы понять, какие признаки больше всего влияют на поведение покупателей. В конце выделил один из сегментов пользователей и разработал персональные предложения для него. Также я добавил информацию о том, какой доход каждый покупатель приносил компании за последние три месяца.| `Python`, `pandas`, `matplotlib`, `seaborn`, `sklearn`, `phik`, `shap` |
| [Использование моделей линейной и логистической регрессии для предсказания вкуса и количества молока, у коров, планируемых к покупке](https://github.com/the13den/yandex_practicum_ds_linear_models_cow_quality_prediction_n4) | В этом проекте, к нам обратился фермер, которому нужно было помочь выбрать лучших коров для покупки. Заводчик и фермер собирали данные о своем поголовье. Заказчик просил выбрать коров, которые приносят как можно больше вкусного молока. В этом проекте я предобработал данные и провёл их исследование с помощью Pandas и библиотек визуализации. Затем отобрал признаки для обучения моделей на основе логической взаимосвязи, корреляций Пирсона и Фи, а также визуального анализа переменных на графике рассеяния. С помощью линейной регрессии получил предсказания количества удоя (в кг). С использованием модели логистической регрессии из библиотеки sklearn, я предсказал бинарную характеристику «молоко вкусное». Разработанные модели достаточно точны и выгодны для практического применения. Также сформированы рекомендации по их улучшению, в том числе на основе анализа остатков. В конце я настроил порог модели логистической регрессии так, чтобы минимизировать ошибку первого рода. Ведь для фермера лучше не купить корову вообще, чем купить корову с невкусным молоком. Результатом работы стал список коров, рекомендованных к покупке.  | `Python`, `Pandas`, `matplotlib`, `Sklearn`, `Numpy` |
| [Статистический анализ данных. Проверка гипотез и моделирование событий для сервиса по прокату самокотов](https://github.com/the13den/yandex_practicum_ds_hypothesis_testing_event_modeling_for_scooter_rent_service_n3) | Задача проекта состояла в проверке гипотез сервиса, описании аудитории и моделировании событий сервиса по прокату самоката. В начале работы у нас были три таблицы с данными сервиса. Я предобработал данные и объединил таблицы согласно требованиям с помощью библиотеки Pandas. Затем провёл исследовательский анализ данных с помощью библиотек matplotlib, seaborn и plotly. В конце, используя библиотеку scipy, я проверил несколько гипотез, смоделировал ряд событий и сформулировал рекомендации для бизнеса. | `Python`, `pandas`, `matplotlib`, `seaborn`, `plotly`, `scipy` |
| Большой проект по предобработке и исследовательскому анализу данных. Выяснение того как господдержка влияет на кассовые сборы и рейтинг фильма | В этом проекте я провел анализ и узнал как господдержка влияет на характеристики фильма. "Большой проект" отличался от двух предыдущих тем, что содержал менее подробное техническое задание. Основная сложность проекта заключалась в большом количестве строковых столбцов, которые необходимо было очищать. В нем я предобработал данные с помощью библиотеки Pandas. Используя графики, таблицы и коэффициент корреляции Пирсона, я провел необходимое исследование. Большая часть визуализаций выполнена с помощью библиотеки Plotly. | `Python`, `Pandas`, `matplotlib`, `plotly` |
| Исследовательский анализ данных. Изучение влияния различных характеристик жилья на его стоимость | В рамках данного проекта нужно было выяснить какие характеристики влияют на стоимость жилья в Санкт-Петербурге. В начале работы я обработал пропуски, переименовал столбцы, удалил дубликаты и добавил новые столбцы в таблицу, используя библиотеку Pandas. Затем выявил и удалил выбросы, ориентируясь на гистограммы и диаграммы размаха (box-plot). В конце, с помощью корреляции Пирсона, графиков и сводных таблиц, описал влияние характеристик на стоимость жилья | `Python`, `Pandas`, `matplotlib`, `seaborn` |
| Предобработка данных. Выявление характеристик заямщиков влияющих на возврат кредита | В этой работе я выявил как различные факторы влияют на возврат кредита. Выполнял работу следующим образом. Используя библиотеку Pandas, я очистил данные от дубликатов, аномалий и пропусков, преобразовал типы данных и добавил новые категориальные признаки. Эта часть проекта была проверена автоматически. Затем с помощью сводных таблиц я провёл необходимый анализ. | `Python`, `Pandas` |
